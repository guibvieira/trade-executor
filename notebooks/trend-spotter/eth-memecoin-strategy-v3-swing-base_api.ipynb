{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Sheet",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Sheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "6dw88EF1XzFFs23BoyC3jb",
     "report_properties": {
      "rowId": "Igzykbf6MEYFRK0y82ue40"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Memecoin Strategy based on Social Data\n",
    "\n",
    "An example strategy backtest loading external social data to drive decision making in this automated memecoin strategy. Enter into trades where there is an uptick in mindshare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "o3AIeQowpLIHBuyIYJ10yt",
     "report_properties": {
      "rowId": "uh5o3Dn5G1utCf1pBXwLXt"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Set up\n",
    "\n",
    "Set up Trading Strategy data client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "dfkqhFAANrPGZ3CKTx6T8w",
     "report_properties": {
      "rowId": "4GwUSsFBuYOKR3ogJzSICm"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tradeexecutor.utils.notebook import setup_charting_and_output\n",
    "from tradingstrategy.client import Client\n",
    "from tradeexecutor.utils.notebook import setup_charting_and_output, OutputMode\n",
    "# import ipdb\n",
    "\n",
    "client = Client.create_jupyter_client()\n",
    "\n",
    "# Set up drawing charts in interactive vector output mode.\n",
    "# This is slower. See the alternative commented option below.\n",
    "# Kernel restart needed if you change output mode.\n",
    "# setup_charting_and_output(OutputMode.interactive)\n",
    "\n",
    "# Set up rendering static PNG images.\n",
    "# This is much faster but disables zoom on any chart.\n",
    "setup_charting_and_output(OutputMode.static, image_format=\"png\", width=1500, height=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "IhzPXJhoXMWwOi1Oeik8RL",
     "report_properties": {
      "rowId": "I8SNSAssB6WzYun0CIP0RF"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "To run this backtest, you first need to run `scripts/prefilter-ethereum.py` to build a Polygon dataset (> 1 GB) for this backtest based on more than 10 GB downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "CyaVFA8LaEDPT7qCQwT8vs",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tradingstrategy.chain import ChainId\n",
    "from tradingstrategy.client import Client\n",
    "from tradingstrategy.timebucket import TimeBucket\n",
    "import datetime\n",
    "\n",
    "liquidity_output_fname = Path(\"/tmp/base-liquidity-prefiltered.parquet\")\n",
    "price_output_fname = Path(\"/tmp/base-price-prefiltered.parquet\")\n",
    "\n",
    "# If the pair does not have this liquidity, skip\n",
    "min_prefilter_liquidity = 10_000\n",
    "\n",
    "chain_id = ChainId.base\n",
    "time_bucket = TimeBucket.d1\n",
    "client = Client.create_jupyter_client()\n",
    "\n",
    "# We need pair metadata to know which pairs belong to Polygon\n",
    "print(\"Downloading/opening pairs dataset\")\n",
    "pairs_df = client.fetch_pair_universe().to_pandas()\n",
    "our_chain_pair_ids = pairs_df[pairs_df.chain_id == chain_id.value][\"pair_id\"].unique()\n",
    "\n",
    "print(f\"We have data for {len(our_chain_pair_ids)} trading pairs on {chain_id.name}\")\n",
    "\n",
    "# Download all liquidity data, extract\n",
    "# trading pairs that exceed our prefiltering threshold\n",
    "print(\"Downloading/opening liquidity dataset\")\n",
    "liquidity_df = client.fetch_all_liquidity_samples(time_bucket).to_pandas()\n",
    "print(f\"Filtering out liquidity for chain {chain_id.name}\")\n",
    "liquidity_df = liquidity_df.loc[liquidity_df.pair_id.isin(our_chain_pair_ids)]\n",
    "liquidity_per_pair = liquidity_df.groupby(liquidity_df.pair_id)\n",
    "print(f\"Chain {chain_id.name} has liquidity data for {len(liquidity_per_pair.groups)}\")\n",
    "\n",
    "liquidity_df.to_parquet(liquidity_output_fname)\n",
    "\n",
    "print(f\"Wrote {liquidity_output_fname}, {liquidity_output_fname.stat().st_size:,} bytes\")\n",
    "\n",
    "print(\"Downloading/opening OHLCV dataset\")\n",
    "price_df = client.fetch_all_candles(time_bucket).to_pandas()\n",
    "price_df.to_parquet(price_output_fname)\n",
    "\n",
    "print(f\"Wrote {price_output_fname}, {price_output_fname.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "ZdpJro1UOPbFw3e3tdw1Nj",
     "report_properties": {
      "rowId": "mO7ovYC34GQbf5pEncLBAK"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# See scripts/prefilter-polygon.py\n",
    "\n",
    "liquidity_output_fname = Path(\"/tmp/base-liquidity-prefiltered.parquet\")\n",
    "price_output_fname = Path(\"/tmp/base-price-prefiltered.parquet\")\n",
    "\n",
    "assert price_output_fname.exists(), \"Run prefilter script first\"\n",
    "assert liquidity_output_fname.exists(), \"Run prefilter script first\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "JCRMy94Ay07bl16HPAxvAz",
     "report_properties": {
      "rowId": "leSUCsgDPZRuTbiwYxqwJo"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Custom data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "yEOofqfBk4Sqnx1UkPIb1m",
     "report_properties": {
      "rowId": "ezL0CmuhAzreuvItUBX6ib"
     },
     "type": "MD"
    }
   },
   "source": [
    "Load the custom data from a CSV file.\n",
    "\n",
    "- Load using Pandas\n",
    "- This data will be split and mapped to per-pair indicators later on, as the data format is per-pair\n",
    "\n",
    "*Note*: Relative paths work different in different notebook run-time environments. Below is for Visual Studio Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Union\n",
    "from datetime import datetime\n",
    "\n",
    "# Global variables\n",
    "API_KEY = \"6a1c9782-436a-47bc-bde6-c883088775a6\"\n",
    "BASE_URL = \"https://api.qa.trendmoon.ai/\"\n",
    "\n",
    "\n",
    "def get_category_coins_scored(category: str = \"Base Meme\", top_n: int = 150) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get scored coins for a specific category from TrendMoon API and enrich with contract addresses.\n",
    "    \n",
    "    Args:\n",
    "        category (str): Category to search for\n",
    "        top_n (int): Number of top coins to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of coin metadata with scores and contract addresses\n",
    "    \"\"\"\n",
    "    # First get the scored data from new endpoint\n",
    "    endpoint = \"get_category_coins\"\n",
    "    \n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Api-key': API_KEY\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'category_name': category,\n",
    "        'top_n': top_n\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{BASE_URL}{endpoint}\",\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "    \n",
    "    scored_data = response.json()\n",
    "    \n",
    "    # Get contract addresses using existing function\n",
    "    coins_with_contracts = get_category_coins(category=category)\n",
    "    \n",
    "    # Create a mapping of coin_id to contract address\n",
    "    contract_map = {\n",
    "        coin['id']: coin['contract_address'] \n",
    "        for coin in coins_with_contracts\n",
    "    }\n",
    "    \n",
    "    # Enrich scored coins with contract addresses\n",
    "    enriched_coins = []\n",
    "    for coin in scored_data['coins']:\n",
    "        coin_data = {\n",
    "            'id': coin['coin_id'],\n",
    "            'date': coin['date'],\n",
    "            'name': coin['name'],\n",
    "            'symbol': coin['symbol'],\n",
    "            'score': coin['score'],\n",
    "            'technical_indicator_score': coin['technical_indicator_score'],\n",
    "            'social_indicator_score': coin['social_indicator_score'],\n",
    "            'day_trend': coin['1_day_trend'],\n",
    "            'day_perc_diff': coin['day_perc_diff'],\n",
    "            'social_mentions': coin['social_mentions'],\n",
    "            'social_dominance': coin['social_dominance'],\n",
    "            'category_relative_social_dominance': coin['category_relative_social_dominance'],\n",
    "            'mentions_ma': coin['mentions_ma'],\n",
    "            'mentions_upper_band': coin['mentions_upper_band'],\n",
    "            'social_ma_crossover': coin['social_ma_crossover'],\n",
    "            'price_above_ma_20': coin['price_above_ma_20'],\n",
    "            'price_above_ma_50': coin['price_above_ma_50'],\n",
    "            'macd_above_signal': coin['macd_above_signal'],\n",
    "            'price_momentum': coin['price_momentum'],\n",
    "            'price_pct_change': coin['price_pct_change'],\n",
    "            'volume_pct_change': coin['volume_pct_change'],\n",
    "            'volume_ratio_20': coin['volume_ratio_20'],\n",
    "            'total_volume': coin['total_volume'],\n",
    "            'market_cap': coin['market_cap'],\n",
    "            'fully_diluted_valuation': coin['fully_diluted_valuation'],\n",
    "            'contract_address': contract_map.get(coin['coin_id'])  # Add contract address from mapping\n",
    "        }\n",
    "        enriched_coins.append(coin_data)\n",
    "    \n",
    "    # Add metadata about the request\n",
    "    result = {\n",
    "        'category_name': scored_data['category_name'],\n",
    "        'date': scored_data['date'],\n",
    "        'data_source': scored_data['data_source'],\n",
    "        'top_n': scored_data['top_n'],\n",
    "        'last_updated': scored_data['last_updated'],\n",
    "        'coins': enriched_coins\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_category_coins(category: str = \"Base Meme\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get coins for a specific category from TrendMoon API.\n",
    "    \n",
    "    Args:\n",
    "        category (str): Category to search for\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of coin metadata\n",
    "    \"\"\"\n",
    "    endpoint = \"coins/search\"\n",
    "    \n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Api-key': API_KEY\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'category': category,\n",
    "        'page': 1,\n",
    "        'page_size': 100\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{BASE_URL}{endpoint}\",\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "    \n",
    "    coins = response.json()\n",
    "    \n",
    "    # Extract only the fields we need\n",
    "    extracted_coins = []\n",
    "    for coin in coins:\n",
    "        extracted_coin = {\n",
    "            'id': coin['id'],\n",
    "            'name': coin['name'],\n",
    "            'symbol': coin['symbol'],\n",
    "            'contract_address': coin['platforms'].get('base'),  # Get Base chain contract address\n",
    "            'market_cap': coin['market_cap'],\n",
    "            'fdv': coin['fully_diluted_valuation']\n",
    "        }\n",
    "        extracted_coins.append(extracted_coin)\n",
    "    \n",
    "    return extracted_coins\n",
    "\n",
    "def get_coin_trends(\n",
    "    coin_ids: List[str],\n",
    "    start_date: str = \"2025-01-01\",\n",
    "    end_date: str = \"2025-05-15\",\n",
    "    interval: str = \"1d\"\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get trends data for specified coins from TrendMoon API.\n",
    "    \n",
    "    Args:\n",
    "        coin_ids (List[str]): List of coin IDs to fetch trends for\n",
    "        start_date (str): Start date in YYYY-MM-DD format\n",
    "        end_date (str): End date in YYYY-MM-DD format\n",
    "        interval (str): Data interval (e.g., \"1d\" for daily)\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary mapping coin IDs to their respective trend data DataFrames\n",
    "    \"\"\"\n",
    "    endpoint = \"social/trends\"\n",
    "    \n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Api-key': API_KEY\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'interval': interval,\n",
    "        'coin_ids': coin_ids  \n",
    "    }\n",
    "\n",
    "    print('params', params)\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{BASE_URL}{endpoint}\",\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "    \n",
    "    trends_data = response.json()\n",
    "    \n",
    "    # Create a dictionary to store DataFrames for each coin\n",
    "    coin_dfs = {}\n",
    "    \n",
    "    for coin_data in trends_data:\n",
    "        coin_id = coin_data['coin_id']\n",
    "        \n",
    "        # Convert trend_market_data to DataFrame\n",
    "        df = pd.DataFrame(coin_data['trend_market_data'])\n",
    "        \n",
    "        # Convert date column to datetime but keep it as a column\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Store DataFrame in dictionary with coin_id as key\n",
    "        coin_dfs[coin_id] = df\n",
    "    \n",
    "    return coin_dfs\n",
    "\n",
    "def merge_coin_trends_data(coins: List[Dict], trends: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge coin metadata with trends data into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        coins (List[Dict]): List of coin metadata from get_category_coins\n",
    "        trends (Dict[str, pd.DataFrame]): Dictionary of trend DataFrames from get_coin_trends\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with MultiIndex (coin_id, date)\n",
    "    \"\"\"\n",
    "    # Create a list to store all the data\n",
    "    all_data = []\n",
    "    \n",
    "    for coin in coins:\n",
    "        coin_id = coin['id']\n",
    "        if coin_id in trends:\n",
    "            df = trends[coin_id].copy()\n",
    "            \n",
    "            # Add coin metadata\n",
    "            df['coin_id'] = coin_id\n",
    "            df['name'] = coin['name']\n",
    "            df['symbol'] = coin['symbol']\n",
    "            df['contract_address'] = coin['contract_address']\n",
    "            \n",
    "            # Append to our list\n",
    "            all_data.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Set MultiIndex\n",
    "    combined_df.set_index(['coin_id', 'date'], inplace=True)\n",
    "    \n",
    "    # Define column order\n",
    "    column_order = [\n",
    "        'sentiment_score', 'symbol_count', 'name_count', 'social_mentions',\n",
    "        'social_dominance', 'price', 'market_cap', 'total_volume',\n",
    "        'lc_posts_active', 'lc_interactions', 'lc_contributors_created',\n",
    "        'lc_contributors_active', 'lc_social_volume_24h',\n",
    "        'day_social_perc_diff', 'hour_social_perc_diff', 'name', 'symbol',\n",
    "        'contract_address'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all columns exist\n",
    "    for col in column_order:\n",
    "        if col not in combined_df.columns:\n",
    "            combined_df[col] = None\n",
    "    \n",
    "    # Reorder columns\n",
    "    combined_df = combined_df[column_order]\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "# Get coins data\n",
    "coins = get_category_coins(category=\"Base Meme\")\n",
    "coin_ids = [coin['id'] for coin in coins]\n",
    "\n",
    "# Get trends data\n",
    "coin_trends = get_coin_trends(coin_ids=coin_ids, start_date=\"2025-01-01\", end_date=\"2025-05-15\", interval=\"1d\")\n",
    "\n",
    "# Merge the data\n",
    "df_trend = merge_coin_trends_data(coins, coin_trends)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "scored_coins = get_category_coins_scored(category=\"Base Meme\", top_n=150)\n",
    "\n",
    "# Access the enriched data\n",
    "for coin in scored_coins['coins']:\n",
    "    print(f\"Name: {coin['name']}\")\n",
    "    print(f\"Score: {coin['score']}\")\n",
    "    print(f\"Contract: {coin['contract_address']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_group.get_group('0xba5e66fb16944da22a62ea4fd70ad02008744460').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = 40\n",
    "\n",
    "\n",
    "latest_data = df_trend[df_trend.index.get_level_values('date') == df_trend.index.get_level_values('date').max()]\n",
    "latest_data.dropna(subset=['contract_address'], inplace=True)\n",
    "token_list_backtest = latest_data.sort_values(by='total_volume', ascending=False)[['symbol', 'contract_address']].iloc[:NUM_TOKENS]\n",
    "\n",
    "\n",
    "base_erc20_address_list = []\n",
    "erc20_addresses_avoid = ['0xA3c322Ad15218fBFAEd26bA7f616249f7705D945'.lower()]\n",
    "base_erc20_address_list += token_list_backtest['contract_address'].tolist()\n",
    "base_erc20_address_list = [address for address in set(base_erc20_address_list) if address.lower() not in erc20_addresses_avoid]\n",
    "print(f\"length of base_erc20_address_list: {len(base_erc20_address_list)}\")\n",
    "# Create per-pair DataFrame group by\n",
    "df_trend = df_trend.reset_index()\n",
    "df_trend['date'] = df_trend['date'].dt.tz_localize(None)\n",
    "df_trend = df_trend.set_index(['coin_id', 'date'])\n",
    "custom_data_group = df_trend.reset_index().set_index('date').sort_index().groupby('contract_address')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5oprFFqPqiyu345uiFL7d0",
     "report_properties": {
      "rowId": "zKwIFYKi3AfbRXufetZTe5"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Parameters\n",
    "\n",
    "- Strategy parameters define the fixed and grid searched parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5l9YiuKxF5OiONN4hiWb0C",
     "report_properties": {
      "rowId": "LYn4zg7uquP5KqCiU61a38"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tradingstrategy.chain import ChainId\n",
    "import datetime\n",
    "\n",
    "from tradeexecutor.strategy.default_routing_options import TradeRouting\n",
    "from tradingstrategy.timebucket import TimeBucket\n",
    "from tradeexecutor.strategy.cycle import CycleDuration\n",
    "from tradeexecutor.strategy.parameters import StrategyParameters\n",
    "\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "class Parameters:\n",
    "    \"\"\"Parameteres for this strategy.\n",
    "\n",
    "    - Collect parameters used for this strategy here\n",
    "\n",
    "    - Both live trading and backtesting parameters\n",
    "    \"\"\"\n",
    "\n",
    "    id = \"base-sentimeme\"  # Used in cache paths\n",
    "\n",
    "    cycle_duration = CycleDuration.d1  # Daily rebalance\n",
    "    candle_time_bucket = TimeBucket.d1\n",
    "    allocation = 0.142\n",
    "    max_assets = 7\n",
    "\n",
    "    #\n",
    "    # Liquidity risk analysis and data quality\n",
    "    #\n",
    "    min_price = 0.00000000000000000001\n",
    "    max_price = 1_000_000\n",
    "    min_liquidity_trade_threshold = 0.05\n",
    "    min_liquidity_threshold = 25000\n",
    "    min_volume = 30000\n",
    "\n",
    "    # Trigger\n",
    "    # Safety Guards\n",
    "    minimum_mometum_threshold = 0.1\n",
    "    momentum_lookback_bars = 9\n",
    "\n",
    "    sma_length = 12\n",
    "    social_ema_short_length = 6\n",
    "    social_ema_long_length = 11\n",
    "    cross_over_period = 2\n",
    "    social_ma_min = 10\n",
    "\n",
    "    stop_loss_pct = 0.92\n",
    "    trailing_stop_loss_pct = 0.86\n",
    "    trailing_stop_loss_activation_level = 1.3\n",
    "\n",
    "    # Trade execution parameters\n",
    "    slippage_tolerance = 0.06\n",
    "    max_buy_tax = 0.06\n",
    "    max_sell_tax = 0.06\n",
    "    token_risk_threshold = 50\n",
    "    # If the pair does not have enough real time quote token TVL, skip trades smaller than this\n",
    "    min_trade_size_usd = 1.00\n",
    "    # Only do trades where we are less than 1% of the pool quote token TVL\n",
    "    per_position_cap_of_pool = 0.01\n",
    "\n",
    "    #\n",
    "    # Live trading only\n",
    "    #\n",
    "    chain_id = ChainId.base\n",
    "    routing = TradeRouting.default\n",
    "    required_history_period = datetime.timedelta(days=30 + 1)\n",
    "\n",
    "    #\n",
    "    # Backtesting only\n",
    "    #\n",
    "    backtest_start = datetime.datetime(2025, 1, 1)\n",
    "    backtest_end = datetime.datetime(2025, 5, 20)\n",
    "    initial_cash = 10_000\n",
    "    initial_deposit = 10_000\n",
    "\n",
    "    stop_loss_time_bucket = TimeBucket.h1\n",
    "\n",
    "\n",
    "parameters = StrategyParameters.from_class(Parameters)  # Convert to AttributedDict to easier typing with dot notation\n",
    "# parameters = StrategyParameters.from_class(Parameters, grid_search=True)  # Convert to AttributedDict to easier typing with dot notation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "tkhLxXXGIHsyGzLI0v5XHU",
     "report_properties": {
      "rowId": "7qHdZrFh01nmPN6AujUGIJ"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Trading pairs and market data\n",
    "\n",
    "- Get a list of ERC-20 tokens we are going to trade on Polygon\n",
    "- Trading pairs are automatically mapped to the best volume /USDC or /WMATIC pair\n",
    "    - Limited to current market information - no historical volume/liquidity analyses performed here\n",
    "- This data loading method caps out at 75 trading pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "cA1pbqYt7PAf75oBNESXL3",
     "report_properties": {
      "rowId": "qrYjxjPLKNFbrO9GBBRmyS"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def filter_pairs_by_risk(\n",
    "    pairs_df: pd.DataFrame,\n",
    "    risk_threshold: int = 60,\n",
    "    max_buy_tax: float = 6.0,\n",
    "    max_sell_tax: float = 6.0,\n",
    "    risk_traits: dict = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter pairs DataFrame based on tax rates, TokenSniffer risk score, and specific risk traits.\n",
    "\n",
    "    Args:\n",
    "        pairs_df (pd.DataFrame): DataFrame containing trading pair information\n",
    "        risk_threshold (int): Minimum acceptable TokenSniffer risk score (0-100, higher is better)\n",
    "        max_buy_tax (float): Maximum allowed buy tax percentage (default 6.0)\n",
    "        max_sell_tax (float): Maximum allowed sell tax percentage (default 6.0)\n",
    "        risk_traits (dict): Dictionary of risk traits to filter on. If None, only tax and risk score are checked\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered pairs DataFrame containing only pairs meeting all criteria\n",
    "\n",
    "    Example Risk Traits Dictionary:\n",
    "    ```python\n",
    "    # Complete risk traits dictionary\n",
    "    risk_traits = {\n",
    "        # Contract-level risks\n",
    "        'has_mint': False,                    # Can new tokens be minted\n",
    "        'has_fee_modifier': False,            # Can fees be modified after deployment\n",
    "        'has_max_transaction_amount': False,   # Presence of max transaction limits\n",
    "        'has_blocklist': False,               # Can addresses be blacklisted\n",
    "        'has_proxy': False,                   # Is it a proxy contract (upgradeable)\n",
    "        'has_pausable': False,                # Can trading be paused\n",
    "\n",
    "        # Ownership and control risks\n",
    "        'is_ownership_renounced': True,       # Ownership should be renounced\n",
    "        'is_source_verified': True,           # Contract should be verified\n",
    "\n",
    "        # Trading risks\n",
    "        'is_sellable': True,                  # Token can be sold\n",
    "        'has_high_buy_fee': False,            # High buy fees present\n",
    "        'has_high_sell_fee': False,           # High sell fees present\n",
    "        'has_extreme_fee': False,             # Extremely high fees (>30%)\n",
    "\n",
    "        # Liquidity risks\n",
    "        'has_inadequate_liquidity': False,    # Insufficient liquidity\n",
    "        'has_inadequate_initial_liquidity': False,  # Started with low liquidity\n",
    "\n",
    "        # Token distribution risks\n",
    "        'has_high_creator_balance': False,    # Creator holds large portion\n",
    "        'has_high_owner_balance': False,      # Owner holds large portion\n",
    "        'has_high_wallet_balance': False,     # Any wallet holds too much\n",
    "        'has_burned_exceeds_supply': False,   # Burned amount > supply (impossible)\n",
    "\n",
    "        # Additional safety checks\n",
    "        'is_flagged': False,                  # Token is flagged for issues\n",
    "        'is_honeypot': False,                 # Known honeypot\n",
    "        'has_restore_ownership': False,       # Can ownership be restored\n",
    "        'has_non_standard_erc20': False       # Non-standard ERC20 implementation\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Example Risk Profiles:\n",
    "    ```python\n",
    "    # Conservative (strict) settings\n",
    "    conservative_risk_traits = {\n",
    "        'has_mint': False,\n",
    "        'has_fee_modifier': False,\n",
    "        'has_blocklist': False,\n",
    "        'has_proxy': False,\n",
    "        'has_pausable': False,\n",
    "        'is_ownership_renounced': True,\n",
    "        'is_source_verified': True,\n",
    "        'is_sellable': True,\n",
    "        'has_high_buy_fee': False,\n",
    "        'has_high_sell_fee': False,\n",
    "        'is_flagged': False,\n",
    "        'is_honeypot': False\n",
    "    }\n",
    "\n",
    "    # Moderate settings\n",
    "    moderate_risk_traits = {\n",
    "        'has_mint': False,\n",
    "        'is_source_verified': True,\n",
    "        'is_sellable': True,\n",
    "        'has_extreme_fee': False,\n",
    "        'is_honeypot': False,\n",
    "        'is_flagged': False\n",
    "    }\n",
    "\n",
    "    # Aggressive settings\n",
    "    aggressive_risk_traits = {\n",
    "        'is_sellable': True,\n",
    "        'is_honeypot': False,\n",
    "        'is_flagged': False\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Usage:\n",
    "    ```python\n",
    "    # Using conservative settings with custom tax limits\n",
    "    filtered_df = filter_pairs_by_risk(\n",
    "        pairs_df,\n",
    "        risk_threshold=60,\n",
    "        max_buy_tax=5.0,\n",
    "        max_sell_tax=5.0,\n",
    "        risk_traits=conservative_risk_traits\n",
    "    )\n",
    "\n",
    "    # Custom risk profile\n",
    "    custom_risk_traits = {\n",
    "        'is_sellable': True,\n",
    "        'is_honeypot': False,\n",
    "        'has_mint': False,\n",
    "        'has_extreme_fee': False,\n",
    "        'is_source_verified': True\n",
    "    }\n",
    "    filtered_df = filter_pairs_by_risk(\n",
    "        pairs_df,\n",
    "        risk_threshold=70,\n",
    "        max_buy_tax=3.0,\n",
    "        max_sell_tax=3.0,\n",
    "        risk_traits=custom_risk_traits\n",
    "    )\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    filtered_df = pairs_df.copy()\n",
    "    initial_count = len(filtered_df)\n",
    "\n",
    "    # Replace NaN values with 0 for buy_tax and sell_tax\n",
    "    filtered_df[\"buy_tax\"] = filtered_df[\"buy_tax\"].fillna(0)\n",
    "    filtered_df[\"sell_tax\"] = filtered_df[\"sell_tax\"].fillna(0)\n",
    "\n",
    "    # Filter for pairs meeting tax thresholds\n",
    "    filtered_df = filtered_df[\n",
    "        (filtered_df[\"buy_tax\"] <= max_buy_tax)\n",
    "        & (filtered_df[\"sell_tax\"] <= max_sell_tax)\n",
    "    ]\n",
    "\n",
    "    after_tax_count = len(filtered_df)\n",
    "    print(f\"After tax filter we have {after_tax_count} trading pairs\")\n",
    "\n",
    "    def check_token_risk(row):\n",
    "        try:\n",
    "            # Extract TokenSniffer data from the nested structure\n",
    "            token_data = row[\"other_data\"][\"top_pair_data\"].token_sniffer_data\n",
    "            if token_data is None:\n",
    "                return False\n",
    "\n",
    "            print(f\"Token data: {token_data}\")\n",
    "            # Check risk score threshold\n",
    "            if token_data.get(\"riskScore\", 0) < risk_threshold:\n",
    "                return False\n",
    "\n",
    "            # Check each specified risk trait if provided\n",
    "            if risk_traits:\n",
    "                for trait, desired_value in risk_traits.items():\n",
    "                    if token_data.get(trait, not desired_value) != desired_value:\n",
    "                        return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        except (KeyError, AttributeError) as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Apply TokenSniffer filters if risk_traits provided\n",
    "    if risk_traits is not None:\n",
    "        filtered_df = filtered_df[filtered_df.apply(check_token_risk, axis=1)]\n",
    "\n",
    "    final_count = len(filtered_df)\n",
    "\n",
    "    print(\n",
    "        \"Filtering results: Initial pairs: %d, after tax filters: %d, after risk filters: %d\",\n",
    "        initial_count,\n",
    "        after_tax_count,\n",
    "        final_count,\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "O5Pk9F7oPhZqjkdVNP3sFs",
     "report_properties": {
      "rowId": "PlFQ2UFrtKb1BRYdZIJ8SH"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tradingstrategy.universe import Universe\n",
    "from tradingstrategy.liquidity import GroupedLiquidityUniverse\n",
    "from tradeexecutor.strategy.pandas_trader.alternative_market_data import resample_multi_pair\n",
    "from tradingstrategy.candle import GroupedCandleUniverse\n",
    "from tradingstrategy.pair import filter_for_base_tokens, PandasPairUniverse, StablecoinFilteringMode, \\\n",
    "    filter_for_stablecoins\n",
    "from tradingstrategy.client import Client\n",
    "\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse, translate_token\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext, notebook_execution_context\n",
    "from tradeexecutor.strategy.universe_model import UniverseOptions\n",
    "from datetime import timedelta\n",
    "from tradingstrategy.pair import (\n",
    "    HumanReadableTradingPairDescription,\n",
    "    PandasPairUniverse,\n",
    "    StablecoinFilteringMode,\n",
    "    filter_for_base_tokens,\n",
    "    filter_for_stablecoins,\n",
    "    filter_for_quote_tokens\n",
    ")\n",
    "from tradingstrategy.timebucket import TimeBucket\n",
    "from tradingstrategy.universe import Universe\n",
    "from tradingstrategy.utils.token_filter import (\n",
    "    deduplicate_pairs_by_volume,\n",
    "    add_base_quote_address_columns,\n",
    ")\n",
    "from tradingstrategy.utils.token_extra_data import load_extra_metadata\n",
    "\n",
    "USDC = \"0x833589fCD6eDb6E08f4c7C32D4f71b54bdA02913\".lower()\n",
    "WETH = \"0x4200000000000000000000000000000000000006\".lower()\n",
    "\n",
    "#If liquidity \n",
    "\n",
    "# We care only Quickswap and Uniswap v3 pairs.\n",
    "# ApeSwap is mostly dead, but listed many bad tokens \n",
    "# in the past, so it is good to include in the sample set.\n",
    "SUPPORTED_DEXES = {\"uniswap-v3\", \"uniswap-v2\", \"sushiswap\", \"aerodrome\"}\n",
    "\n",
    "\n",
    "avoid_backtesting_tokens = {\n",
    "    # Trading jsut stops (though there is liq left)\n",
    "    # https://tradingstrategy.ai/trading-view/ethereum/uniswap-v3/id-usdc-fee-30\n",
    "    \"PEOPLE\",\n",
    "    \"WBTC\"\n",
    "}\n",
    "\n",
    "# Get the token list of everything in the CSV + hardcoded WMATIC\n",
    "custom_data_token_set = {WETH} | set(base_erc20_address_list)\n",
    "\n",
    "\n",
    "def create_trading_universe(\n",
    "    timestamp: datetime.datetime,\n",
    "    client: Client,\n",
    "    execution_context: ExecutionContext,\n",
    "    universe_options: UniverseOptions,\n",
    ") -> TradingStrategyUniverse:\n",
    "    \"\"\"Create the trading universe.\"\"\"\n",
    "    start_at = parameters.backtest_start\n",
    "    end_at = parameters.backtest_end\n",
    "\n",
    "    print(f\"Backtesting {start_at} - {end_at}\")\n",
    "\n",
    "    chain_id = Parameters.chain_id\n",
    "\n",
    "    exchange_universe = client.fetch_exchange_universe()\n",
    "    \n",
    "    exchange_universe = exchange_universe.limit_to_chains({Parameters.chain_id}).limit_to_slugs(SUPPORTED_DEXES)\n",
    "    print(f\"Exchange universe: {exchange_universe.get_all_slugs()}\")\n",
    "    print(f\"We support {exchange_universe.get_exchange_count()} DEXes\")\n",
    "    \n",
    "# 6239\n",
    "# 6294    21461 uniswap v2\n",
    "# 6091     1135 baseswap\n",
    "# 6179      482 sushiswap\n",
    "# 6051      392\n",
    "\n",
    "\n",
    "    pairs_df = client.fetch_pair_universe().to_pandas()\n",
    "\n",
    "    liquidity_df = pd.read_parquet(liquidity_output_fname)\n",
    "    price_df = pd.read_parquet(price_output_fname)\n",
    "    print(f\"Length of price_df: {len(price_df)}\")\n",
    "\n",
    "    # # When reading from Parquet file, we need to deal with indexing by hand\n",
    "    liquidity_df.index = pd.DatetimeIndex(liquidity_df.timestamp)\n",
    "    price_df.index = pd.DatetimeIndex(price_df.timestamp)\n",
    "\n",
    "    print(f\"Prefilter data contains {len(liquidity_df):,} liquidity samples dn {len(price_df):,} OHLCV candles\")\n",
    "    print(f\"type of start_at: {type(start_at)}\")\n",
    "    print(f\"type of end_at: {type(end_at)}\")\n",
    "\n",
    "    # Crop price and liquidity data to our backtesting range\n",
    "    price_df = price_df.loc[(price_df.timestamp >= start_at) & (price_df.timestamp <= end_at)]\n",
    "    print(f\"Price data range {price_df.timestamp.min()} - {price_df.timestamp.max()}\")\n",
    "    liquidity_df = liquidity_df.loc[(liquidity_df.timestamp >= start_at) & (liquidity_df.timestamp <= end_at)]\n",
    "\n",
    "    # Prefilter for more liquidity conditions\n",
    "    liquidity_per_pair = liquidity_df.groupby(liquidity_df.pair_id)\n",
    "    print(f\"Chain {chain_id.name} has liquidity data for {len(liquidity_per_pair.groups)}\")\n",
    "\n",
    "    #Uncomment for highest liqudiity pairs\n",
    "    # passed_pair_ids = set()\n",
    "    # for pair_id, pair_df in liquidity_per_pair:\n",
    "    #     if pair_df[\"high\"].max() > Parameters.min_liquidity_threshold:\n",
    "    #         passed_pair_ids.add(pair_id)\n",
    "\n",
    "    # Get the date 30 days ago\n",
    "    thirty_days_ago = end_at - timedelta(days=30)\n",
    "\n",
    "    # Create a subset of the data for the last 30 days, in live trading this makes more sense\n",
    "    # liquidity_last_30_days = liquidity_df #liquidity_df[liquidity_df['timestamp'] > thirty_days_ago]\n",
    "    # liquidity_per_pair_last_30d = liquidity_last_30_days.groupby('pair_id')\n",
    "    # passed_pair_ids = set()\n",
    "    # for pair_id, pair_df in liquidity_per_pair_last_30d:\n",
    "    #     # Check the maximum high liquidity in the last 30 days\n",
    "    #     if pair_df[\"high\"].max() > Parameters.min_liquidity_threshold:\n",
    "    #         passed_pair_ids.add(pair_id)\n",
    "\n",
    "    # pairs_df = pairs_df.loc[pairs_df.pair_id.isin(passed_pair_ids)]\n",
    "\n",
    "    # pairs_df.to_csv('pairs_df_post_liquidity_filter.csv')\n",
    "    # print(f\"There are {len(passed_pair_ids)} after liquidity filter\")\n",
    "    print(f\"After liquidity filter {Parameters.min_liquidity_threshold:,} USD we have {len(pairs_df)} trading pairs\")\n",
    "\n",
    "\n",
    "    # allowed_exchange_ids = set(exchange_universe.exchanges.keys()) | {6294}\n",
    "    allowed_exchange_ids = set(exchange_universe.exchanges.keys())\n",
    "    print(f\"Allowed exchange ids: {exchange_universe.exchanges.keys()}\")\n",
    "    pairs_df.to_csv('pairs_df_pre_exchange_filter.csv')\n",
    "    pairs_df = pairs_df.loc[pairs_df.exchange_id.isin(allowed_exchange_ids)]\n",
    "    print(f\"After DEX filter we have {len(pairs_df)} trading pairs\")\n",
    "    pairs_df.to_csv('pairs_df_after_exchange_filter.csv')\n",
    "\n",
    "    # Store reference USDC ETH pair so we have an example pair with USDC as quote token for reserve asset\n",
    "    # Get ETH-USDC pairs from all major DEXes\n",
    "    eth_usdc_addresses = [\n",
    "        \"0x88A43bbDF9D098eEC7bCEda4e2494615dfD9bB9C\",  # Uniswap V2,\n",
    "        \"0xd0b53D9277642d899DF5C87A3966A349A798F224\"  # Uniswap V3\n",
    "    ]\n",
    "    print(f\"ETH-USDC pairs: {eth_usdc_addresses}\")\n",
    "    \n",
    "    ref_usdc_pairs = pairs_df[\n",
    "        pairs_df[\"address\"].isin([addr.lower() for addr in eth_usdc_addresses])\n",
    "    ].copy()\n",
    "\n",
    "    # Pairs pre-processing\n",
    "    pairs_df = add_base_quote_address_columns(pairs_df)\n",
    "    pairs_df = pairs_df.loc[\n",
    "        (pairs_df[\"base_token_address\"].isin(base_erc20_address_list))\n",
    "        & (pairs_df[\"chain_id\"] == chain_id)\n",
    "    ]\n",
    "\n",
    "    pairs_df = filter_for_base_tokens(pairs_df, list(base_erc20_address_list))\n",
    "\n",
    "    print(f\"Before deduplication we have {len(pairs_df)} trading pairs\")\n",
    "    pairs_df = deduplicate_pairs_by_volume(pairs_df)\n",
    "    print(f\"After deduplication we have {len(pairs_df)} trading pairs\")\n",
    "\n",
    "    # Retrofit TokenSniffer data\n",
    "    pairs_df = load_extra_metadata(\n",
    "        pairs_df,\n",
    "        client=client,\n",
    "    )\n",
    "\n",
    "    pairs_df = filter_pairs_by_risk(\n",
    "        pairs_df,\n",
    "        risk_threshold=Parameters.token_risk_threshold,\n",
    "        max_buy_tax=Parameters.max_buy_tax,\n",
    "        max_sell_tax=Parameters.max_sell_tax,\n",
    "        risk_traits=None,\n",
    "    )\n",
    "\n",
    "    pairs_df = filter_for_stablecoins(pairs_df, StablecoinFilteringMode.only_volatile_pairs)\n",
    "    print(f\"After custom data ERC-20 token address cross section filter we have {len(pairs_df)} matching trading pairs\")\n",
    "    \n",
    "    # We want to keep only USDC or WETH quoted pairs\n",
    "    USDC = \"0x833589fCD6eDb6E08f4c7C32D4f71b54bdA02913\".lower()\n",
    "    WETH = \"0x4200000000000000000000000000000000000006\".lower()\n",
    "    pairs_df = filter_for_quote_tokens(\n",
    "        pairs_df,\n",
    "        {\n",
    "            USDC,\n",
    "            WETH,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ref_usdc_pairs.to_csv('ref_usdc_pairs.csv')\n",
    "\n",
    "    print(f\"Before deduplication we have {len(pairs_df)} trading pairs\")\n",
    "    pairs_df = deduplicate_pairs_by_volume(pairs_df)\n",
    "    print(f\"After deduplication we have {len(pairs_df)} trading pairs\")\n",
    "\n",
    "    pairs_df = pd.concat([pairs_df, ref_usdc_pairs]).drop_duplicates(subset=[\"pair_id\"])\n",
    "    print(f\"After adding ref USDC pairs we have {len(pairs_df)} trading pairs\")\n",
    "\n",
    "    pairs_df.to_csv('pairs_df_base_quote_token_filter.csv')\n",
    "    \n",
    "\n",
    "    # Resample strategy decision candles to daily\n",
    "    daily_candles = resample_multi_pair(price_df, Parameters.candle_time_bucket)\n",
    "    daily_candles[\"timestamp\"] = daily_candles.index\n",
    "\n",
    "    print(f\"After downsampling we have {len(daily_candles)} OHLCV candles and {len(liquidity_df)} liquidity samples\")\n",
    "    candle_universe = GroupedCandleUniverse(\n",
    "        daily_candles,\n",
    "        time_bucket=Parameters.candle_time_bucket,\n",
    "        forward_fill=True  # Forward will should make sure we can always calculate RSI, other indicators\n",
    "    )\n",
    "\n",
    "    liquidity_universe = GroupedLiquidityUniverse(liquidity_df)\n",
    "\n",
    "    # The final trading pair universe contains metadata only for pairs that passed\n",
    "    # our filters\n",
    "    pairs_universe = PandasPairUniverse(pairs_df, exchange_universe=exchange_universe)\n",
    "    stop_loss_candle_universe = GroupedCandleUniverse(price_df)\n",
    "\n",
    "    data_universe = Universe(\n",
    "        time_bucket=Parameters.candle_time_bucket,\n",
    "        # liquidity_time_bucket=Parameters.candle_time_bucket,\n",
    "        exchange_universe=exchange_universe,\n",
    "        pairs=pairs_universe,\n",
    "        candles=candle_universe,\n",
    "        liquidity=liquidity_universe,\n",
    "        chains={Parameters.chain_id},\n",
    "        forward_filled=True,\n",
    "    )\n",
    "\n",
    "    reserve_asset = translate_token(pairs_universe.get_token(USDC))\n",
    "\n",
    "    _strategy_universe = TradingStrategyUniverse(\n",
    "        data_universe=data_universe,\n",
    "        backtest_stop_loss_time_bucket=Parameters.stop_loss_time_bucket,\n",
    "        backtest_stop_loss_candles=stop_loss_candle_universe,\n",
    "        reserve_assets={reserve_asset},\n",
    "        price_data_delay_tolerance=pd.Timedelta(days=7)\n",
    "    )\n",
    "\n",
    "    return _strategy_universe, pairs_universe\n",
    "\n",
    "\n",
    "strategy_universe, pairs_universe = create_trading_universe(\n",
    "    None,\n",
    "    client,\n",
    "    notebook_execution_context,\n",
    "    UniverseOptions.from_strategy_parameters_class(Parameters, notebook_execution_context)\n",
    "    \n",
    ")\n",
    "\n",
    "broken_trading_pairs = set()\n",
    "\n",
    "#\n",
    "# Extra sanity checks\n",
    "# \n",
    "# Ru some extra sanity check for small cap tokens\n",
    "#\n",
    "\n",
    "print(\"Checking trading pair quality\")\n",
    "print(\"-\" * 80)\n",
    "pairs_to_avoid = [87449]\n",
    "\n",
    "for pair in strategy_universe.iterate_pairs():\n",
    "    reason = strategy_universe.get_trading_broken_reason(pair, min_candles_required=10, min_price=parameters.min_price, max_price=parameters.max_price)\n",
    "    if pair.internal_id in pairs_to_avoid:\n",
    "        broken_trading_pairs.add(pair)\n",
    "    if reason:\n",
    "        print(f\"FAIL: {pair} with base token {pair.base.address} may be problematic: {reason}\")\n",
    "        broken_trading_pairs.add(pair)\n",
    "    else:\n",
    "        print(f\"OK: {pair} included in the backtest\")\n",
    "\n",
    "print(f\"Total {len(broken_trading_pairs)} broken trading pairs detected, having {strategy_universe.get_pair_count() - len(broken_trading_pairs)} good pairs left to trade\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "8A5KpJd9246eT05kf2S0zS",
     "report_properties": {
      "rowId": "FZSXhS5AxXpEv18zirqLFE"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Indicators\n",
    "\n",
    "- We use `pandas_ta` Python package to calculate technical indicators\n",
    "- These indicators are precalculated and cached on the disk\n",
    "- This includes caching our custom made indicators, so we only calculate them once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "d8rZrXcH4iRRRBoCOH9MbG",
     "report_properties": {
      "rowId": "dj5kjQzQ1hYacmyw6Mbh0K"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta\n",
    "import datetime\n",
    "from tradeexecutor.analysis.regime import Regime\n",
    "from tradeexecutor.state.identifier import TradingPairIdentifier\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import IndicatorSet, IndicatorSource\n",
    "from tradeexecutor.strategy.parameters import StrategyParameters\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse\n",
    "from tradeexecutor.utils.crossover import contains_cross_over, contains_cross_under\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_metric(pair: TradingPairIdentifier, metric_name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Add a specific metric to the dataset, filling in missing dates and forward filling values.\n",
    "    Handles duplicate dates by summing the values.\n",
    "    \"\"\"\n",
    "    contract_address = pair.base.address\n",
    "    try:\n",
    "        contract_address = pair.base.address\n",
    "        per_pair = custom_data_group.get_group(contract_address)\n",
    "        per_pair_series = per_pair[metric_name]\n",
    "        metric_series = per_pair_series[\n",
    "            ~per_pair_series.index.duplicated(keep=\"last\")\n",
    "        ].rename(metric_name)\n",
    "        full_date_range = pd.date_range(\n",
    "            start=metric_series.index.min(), end=metric_series.index.max(), freq=\"D\"\n",
    "        )\n",
    "        metric_series = metric_series.reindex(full_date_range)\n",
    "        metric_series = metric_series.ffill()\n",
    "        metric_series.sort_index(inplace=True)\n",
    "        return metric_series\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding metric {metric_name} for pair {pair}: {str(e)}\")\n",
    "        return pd.Series(dtype=\"float64\", index=pd.DatetimeIndex([]))\n",
    "\n",
    "def calculate_metric_emas(pair: TradingPairIdentifier, metric_name: str, short_length: int, long_length: int) -> pd.DataFrame:\n",
    "    \"\"\"Calculate short and long EMAs for a specific metric.\"\"\"\n",
    "    metric_series = add_metric(pair, metric_name)\n",
    "    metric_series = metric_series.interpolate(method='linear', limit_direction='forward')\n",
    "    ema_short = metric_series.ewm(span=short_length, adjust=False).mean().rename(f\"{metric_name}_ema_short\")\n",
    "    ema_long = metric_series.ewm(span=long_length, adjust=False).mean().rename(f\"{metric_name}_ema_long\")\n",
    "    emas_df = pd.concat([ema_short, ema_long], axis=1)\n",
    "    return emas_df\n",
    "\n",
    "\n",
    "def calculate_metric_bbands(pair: TradingPairIdentifier, metric_name: str, length: int, std: int) -> pd.DataFrame:\n",
    "    \"\"\"Calculate Bollinger Bands for a specific metric.\"\"\"\n",
    "    try:\n",
    "        metric_series = add_metric(pair, metric_name)\n",
    "        metric_series.sort_index(inplace=True)\n",
    "        metric_series = metric_series.interpolate(method='linear', limit_direction='forward')\n",
    "        \n",
    "        print(f\"Length of metric series for {metric_name}: {len(metric_series)}\")\n",
    "        \n",
    "        # Calculate Bollinger Bands\n",
    "        bb = pandas_ta.bbands(metric_series, length=length, std=std)\n",
    "        bb = bb.rename(columns={\n",
    "            f'BBL_{length}_{std}': f'{metric_name}_BBL_{length}_{std}',\n",
    "            f'BBM_{length}_{std}': f'{metric_name}_BBM_{length}_{std}',\n",
    "            f'BBU_{length}_{std}': f'{metric_name}_BBU_{length}_{std}',\n",
    "            f'BBB_{length}_{std}': f'{metric_name}_BBB_{length}_{std}',\n",
    "            f'BBP_{length}_{std}': f'{metric_name}_BBP_{length}_{std}'\n",
    "        })\n",
    "        \n",
    "        return bb\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Bollinger Bands for {metric_name}: {str(e)}\")\n",
    "        # Return an empty DataFrame in case of error\n",
    "        return pd.Series(dtype=\"float64\", index=pd.DatetimeIndex([]))\n",
    "    \n",
    "def momentum(close, momentum_lookback_bars) -> pd.Series:\n",
    "    \"\"\"Calculate momentum series to be used as a signal.\n",
    "\n",
    "    This indicator is later processed in decide_trades() to a weighted alpha signal.\n",
    "    \n",
    "    :param momentum_lookback_bars:\n",
    "        Calculate returns based on this many bars looked back\n",
    "    \"\"\"\n",
    "    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html#pandas.DataFrame.shift\n",
    "    start_close = close.shift(momentum_lookback_bars)\n",
    "    momentum = (close - start_close) / start_close\n",
    "    return momentum\n",
    "\n",
    "\n",
    "def calculate_macd(price_data: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.DataFrame:\n",
    "    # Calculate the MACD and signal line\n",
    "    exp1 = price_data.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = price_data.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = exp1 - exp2\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'macd': macd_line,\n",
    "        'signal': signal_line,\n",
    "        'histogram': histogram\n",
    "    })\n",
    "\n",
    "def calculate_rsi(close, length=14):\n",
    "    \"\"\"Calculate RSI\"\"\"\n",
    "    delta = close.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=length).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=length).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_obv(close: pd.Series, volume: pd.Series) -> pd.Series:\n",
    "    \"\"\"Calculates the On Balance Volume (OBV) indicator.\n",
    "    \n",
    "    Args:\n",
    "        close: Series of closing prices\n",
    "        volume: Series of volume values\n",
    "        \n",
    "    Logic:\n",
    "        - First point: OBV = Volume\n",
    "        - If close price increases: OBV = previous OBV + Volume\n",
    "        - If close price decreases: OBV = previous OBV - Volume\n",
    "        - If close price unchanged: OBV = previous OBV\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: OBV values with same index as inputs\n",
    "    \"\"\"\n",
    "    assert len(close) == len(volume), \"Close prices and volume must have same length\"\n",
    "    assert all(volume > 0), \"Volume must be positive values\"\n",
    "\n",
    "    obv = pd.Series(index=close.index, dtype=float)\n",
    "    obv.iloc[0] = volume.iloc[0]\n",
    "    \n",
    "    for i in range(1, len(close)):\n",
    "        if close.iloc[i] > close.iloc[i-1]:\n",
    "            obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]\n",
    "        elif close.iloc[i] < close.iloc[i-1]:\n",
    "            obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]\n",
    "        else:\n",
    "            obv.iloc[i] = obv.iloc[i-1]\n",
    "    \n",
    "    return obv\n",
    "\n",
    "def create_indicators(\n",
    "    timestamp: datetime.datetime | None,\n",
    "    parameters: StrategyParameters,\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    "    execution_context: ExecutionContext\n",
    "):\n",
    "    indicators = IndicatorSet()\n",
    "\n",
    "    indicators.add(\n",
    "        \"momentum\",\n",
    "        momentum,\n",
    "        {\"momentum_lookback_bars\": parameters.momentum_lookback_bars},\n",
    "        IndicatorSource.close_price,\n",
    "    ) \n",
    "\n",
    "    indicators.add(\n",
    "        \"macd\",\n",
    "        calculate_macd,\n",
    "        {\"fast\": 12, \"slow\": 26, \"signal\": 9},\n",
    "        IndicatorSource.close_price,\n",
    "    )\n",
    "\n",
    "    # Add RSI\n",
    "    indicators.add(\n",
    "        \"rsi\",\n",
    "        calculate_rsi,\n",
    "        {\"length\": 14},\n",
    "        IndicatorSource.close_price,\n",
    "    )\n",
    "\n",
    "    # Add OBV\n",
    "    # indicators.add(\n",
    "    #     \"obv\",\n",
    "    #     calculate_obv,\n",
    "    #     {},\n",
    "    #     IndicatorSource.ohlcv,\n",
    "    # )\n",
    "\n",
    "\n",
    "    #Social Metrics\n",
    "    # Add original value indicators\n",
    "    social_metrics = [\n",
    "        \"social_mentions\"\n",
    "    ]\n",
    "\n",
    "    for metric in social_metrics:\n",
    "        print('social_metric going for', metric)\n",
    "        indicators.add(\n",
    "            metric,\n",
    "            add_metric,\n",
    "            {\"metric_name\": metric},  # Pass the metric name as a parameter\n",
    "            IndicatorSource.external_per_pair,\n",
    "        )\n",
    "\n",
    "        # Add EMA indicators\n",
    "        indicators.add(\n",
    "            f\"{metric}_emas\",\n",
    "            calculate_metric_emas,\n",
    "            {\n",
    "                \"metric_name\": metric,\n",
    "                \"short_length\": parameters.social_ema_short_length,\n",
    "                \"long_length\": parameters.social_ema_long_length\n",
    "            },\n",
    "            IndicatorSource.external_per_pair,\n",
    "        )\n",
    "\n",
    "    return indicators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "0ToTB02EcQsCXlnK6z2Yy7",
     "report_properties": {
      "rowId": "Z9YQ4OHii5m2LZuDzG4X7T"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Trading algorithm\n",
    "\n",
    "- Describe out trading strategy as code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Q4XFF2DzXAe8jrZ0D0iGlK",
     "report_properties": {
      "rowId": "swjxViRqShK0nDPy6OnSz4"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tradeexecutor.state.visualisation import PlotKind\n",
    "from tradeexecutor.state.trade import TradeExecution\n",
    "from tradeexecutor.strategy.pandas_trader.strategy_input import StrategyInput, StrategyInputIndicators\n",
    "from tradeexecutor.strategy.weighting import weight_by_1_slash_n\n",
    "from tradeexecutor.strategy.alpha_model import AlphaModel\n",
    "from tradeexecutor.strategy.pandas_trader.strategy_input import IndicatorDataNotFoundWithinDataTolerance\n",
    "from tradeexecutor.utils.crossover import contains_cross_over, contains_cross_under\n",
    "from pandas_ta.overlap import ema\n",
    "from decimal import Decimal\n",
    "from tradeexecutor.state.trigger import TriggerType, TriggerCondition\n",
    "from tradeexecutor.state.identifier import TradingPairIdentifier\n",
    "import cachetools\n",
    "\n",
    "\n",
    "from tradeexecutor.strategy.tvl_size_risk import USDTVLSizeRiskModel\n",
    "import re\n",
    "\n",
    "def update_tp_level(tp_levels_reached, position_id, tp_level, reached=True):\n",
    "    \"\"\"\n",
    "    Update the TP level reached status for a given position in the provided dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - tp_levels_reached: Dictionary tracking TP levels for positions.\n",
    "    - position_id: Unique identifier for the position.\n",
    "    - tp_level: The TP level being updated (e.g., 'tp1', 'tp2').\n",
    "    - reached: Boolean indicating whether the TP level has been reached.\n",
    "    \"\"\"\n",
    "    if position_id not in tp_levels_reached:\n",
    "        tp_levels_reached[position_id] = {}\n",
    "    tp_levels_reached[position_id][tp_level] = reached\n",
    "\n",
    "def is_tp_level_reached(tp_levels_reached, position_id, tp_level):\n",
    "    \"\"\"\n",
    "    Check if a TP level has been reached for a given position in the provided dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - tp_levels_reached: Dictionary tracking TP levels for positions.\n",
    "    - position_id: Unique identifier for the position.\n",
    "    - tp_level: The TP level to check (e.g., 'tp1', 'tp2').\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean indicating whether the TP level has been reached.\n",
    "    \"\"\"\n",
    "    return tp_levels_reached.get(position_id, {}).get(tp_level, False)\n",
    "\n",
    "\n",
    "def is_volume_increasing(current_volume, volume_sma):\n",
    "    \"\"\"Check if the current volume is significantly higher than the SMA.\"\"\"\n",
    "    return current_volume > volume_sma\n",
    "\n",
    "\n",
    "tp_levels_reached = {}\n",
    "\n",
    "def generate_visualisations(timestamp, pair, social_data_indicators, visualisation, metrics):\n",
    "    for metric in metrics:\n",
    "        visualisation.plot_indicator(\n",
    "            timestamp, \n",
    "            f\"{metric} {pair.base}\", \n",
    "            PlotKind.technical_indicator_detached, \n",
    "            social_data_indicators[f\"{metric}_ema_short\"], \n",
    "            pair=pair\n",
    "            # detached_overlay_name=f\"{metric} {pair.base}\"\n",
    "        )\n",
    "\n",
    "        # Plot the long EMA overlay\n",
    "        visualisation.plot_indicator(\n",
    "            timestamp, \n",
    "            f\"Long EMA {metric} {pair.base}\", \n",
    "            PlotKind.technical_indicator_overlay_on_detached, \n",
    "            social_data_indicators[f\"{metric}_ema_long\"], \n",
    "            pair=pair, \n",
    "            detached_overlay_name=f\"{metric} {pair.base}\"\n",
    "        )\n",
    "\n",
    "def is_accetable(\n",
    "    indicators: StrategyInputIndicators,\n",
    "    parameters: StrategyParameters,\n",
    "    pair: TradingPairIdentifier,\n",
    "    momentum: float | None,\n",
    ") -> bool:\n",
    "    \"\"\"Check the pair for risk acceptance\n",
    "\n",
    "    :return:\n",
    "        True if we should trade this pair\n",
    "    \"\"\"\n",
    "\n",
    "    if pair in broken_trading_pairs:\n",
    "        # Don't even bother to try trade this\n",
    "        return False\n",
    "\n",
    "    if pair.base.token_symbol in avoid_backtesting_tokens:\n",
    "        # Manually blacklisted toen for this backtest\n",
    "        return False\n",
    "\n",
    "    # Pair does not quality yet due to low liquidity\n",
    "    liquidity = indicators.get_tvl(pair=pair)\n",
    "    if liquidity is None or liquidity <= parameters.min_liquidity_threshold:\n",
    "        return False\n",
    "\n",
    "    volume = indicators.get_price(pair, column=\"volume\")\n",
    "    close_price = indicators.get_price(pair=pair)\n",
    "    volume_adjusted = volume / close_price\n",
    "\n",
    "    if volume_adjusted < parameters.min_volume:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def calculate_market_confidence(\n",
    "    macd: float,\n",
    "    signal_line: float,\n",
    "    rsi: float,\n",
    "    momentum: float,\n",
    "    obv_slope: float,\n",
    "    parameters: StrategyParameters\n",
    ") -> tuple[float, dict]:\n",
    "    \"\"\"Calculate market confidence score based on technical indicators.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[float, dict]: Confidence score (0-1) and detailed traits analysis\n",
    "    \"\"\"\n",
    "    traits = {\n",
    "        \"macd_above_signal\": {\n",
    "            \"condition\": macd > signal_line,\n",
    "            \"weight\": 0.25,\n",
    "            \"description\": \"MACD above signal line\"\n",
    "        },\n",
    "        \"rsi_momentum\": {\n",
    "            \"condition\": rsi > parameters.rsi_threshold,\n",
    "            \"weight\": 0.25,\n",
    "            \"description\": \"RSI showing momentum\"\n",
    "        },\n",
    "        \"price_momentum\": {\n",
    "            \"condition\": momentum > parameters.minimum_mometum_threshold,\n",
    "            \"weight\": 0.25,\n",
    "            \"description\": \"Strong price momentum\"\n",
    "        },\n",
    "        \"obv_uptrend\": {\n",
    "            \"condition\": obv_slope > 0,\n",
    "            \"weight\": 0.25,\n",
    "            \"description\": \"Rising OBV\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate confidence score\n",
    "    confidence_score = sum(\n",
    "        trait[\"weight\"] for trait in traits.values() \n",
    "        if trait[\"condition\"]\n",
    "    )\n",
    "    \n",
    "    # Add the actual values to the traits dict for logging\n",
    "    traits[\"macd_above_signal\"][\"value\"] = float(macd - signal_line)\n",
    "    traits[\"rsi_momentum\"][\"value\"] = float(rsi)\n",
    "    traits[\"price_momentum\"][\"value\"] = float(momentum)\n",
    "    traits[\"obv_uptrend\"][\"value\"] = float(obv_slope)\n",
    "    \n",
    "    return confidence_score, traits\n",
    "\n",
    "\n",
    "def safe_get_indicator(indicators, indicator_name, pair, column=None, default_value=None, tolerance_days=15):\n",
    "    \"\"\"Safely get an indicator value with error handling and tolerance.\n",
    "    \n",
    "    Args:\n",
    "        indicators: StrategyInputIndicators instance\n",
    "        indicator_name: Name of the indicator to fetch\n",
    "        pair: Trading pair\n",
    "        column: Specific column name for multi-column indicators\n",
    "        default_value: Value to return if indicator fetch fails\n",
    "        tolerance_days: Data delay tolerance in days\n",
    "    \n",
    "    Returns:\n",
    "        Indicator value or default_value if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if column:\n",
    "            return indicators.get_indicator_value(\n",
    "                indicator_name, \n",
    "                pair=pair, \n",
    "                column=column,\n",
    "                data_delay_tolerance=pd.Timedelta(days=tolerance_days)\n",
    "            )\n",
    "        else:\n",
    "            return indicators.get_indicator_value(\n",
    "                indicator_name, \n",
    "                pair=pair,\n",
    "                data_delay_tolerance=pd.Timedelta(days=tolerance_days)\n",
    "            )\n",
    "    except (IndicatorDataNotFoundWithinDataTolerance, Exception) as e:\n",
    "        print(f\"Warning: Could not fetch indicator {indicator_name} for {pair}: {str(e)}\")\n",
    "        return default_value\n",
    "    \n",
    "def safe_get_indicator_series(indicators, indicator_name, pair, column=None, default_value=None, tolerance_days=15):\n",
    "    \"\"\"Safely get an indicator series with error handling.\n",
    "    \n",
    "    Args:\n",
    "        indicators: StrategyInputIndicators instance\n",
    "        indicator_name: Name of the indicator to fetch\n",
    "        pair: Trading pair\n",
    "        column: Specific column name for multi-column indicators\n",
    "        default_value: Value to return if indicator fetch fails (typically empty series)\n",
    "        tolerance_days: Data delay tolerance in days\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Indicator series or default_value (empty series) if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if column:\n",
    "            return indicators.get_indicator_series(\n",
    "                indicator_name, \n",
    "                pair=pair, \n",
    "                column=column,\n",
    "                data_delay_tolerance=pd.Timedelta(days=tolerance_days)\n",
    "            )\n",
    "        else:\n",
    "            return indicators.get_indicator_series(\n",
    "                indicator_name, \n",
    "                pair=pair,\n",
    "                data_delay_tolerance=pd.Timedelta(days=tolerance_days)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not fetch indicator series {indicator_name} for {pair}: {str(e)}\")\n",
    "        return pd.Series() if default_value is None else default_value\n",
    "    \n",
    "\n",
    "def safe_get_price(indicators, pair, column=\"close\", default_value=None, tolerance_days=7):\n",
    "    \"\"\"Safely get price data with error handling and tolerance.\n",
    "    \n",
    "    Args:\n",
    "        indicators: StrategyInputIndicators instance\n",
    "        pair: Trading pair\n",
    "        column: Price column to fetch (close, open, high, low, volume)\n",
    "        default_value: Value to return if price fetch fails\n",
    "        tolerance_days: Data lag tolerance in days\n",
    "    \n",
    "    Returns:\n",
    "        Price value or default_value if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return indicators.get_price(\n",
    "            pair=pair,\n",
    "            column=column,\n",
    "            data_lag_tolerance=pd.Timedelta(days=tolerance_days)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not fetch {column} price for {pair}: {str(e)}\")\n",
    "        return default_value\n",
    "\n",
    "def safe_get_tvl(indicators, pair, default_value=None, tolerance_days=7):\n",
    "    \"\"\"Safely get TVL data with error handling and tolerance.\"\"\"\n",
    "    try:\n",
    "        return indicators.get_tvl(\n",
    "            pair=pair,\n",
    "            data_lag_tolerance=pd.Timedelta(days=tolerance_days)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not fetch TVL for {pair}: {str(e)}\")\n",
    "        return default_value\n",
    "    \n",
    "#Logging helpers\n",
    "def strip_except_newlines(text):\n",
    "    # Split by newlines, strip each line, then rejoin\n",
    "    return '\\n'.join(line.strip() for line in text.splitlines())\n",
    "\n",
    "def dedent_any(text: str) -> str:\n",
    "    \"\"\"Dedent variable indents of the text\"\"\"\n",
    "    return re.sub(r'^\\s+', '', strip_except_newlines(text), flags=re.MULTILINE)\n",
    "\n",
    "def is_acceptable(\n",
    "    indicators: StrategyInputIndicators,\n",
    "    parameters: StrategyParameters,\n",
    "    pair: TradingPairIdentifier\n",
    ") -> bool:\n",
    "    \"\"\"Check the pair for risk acceptance\n",
    "\n",
    "    :return:\n",
    "        True if we should trade this pair\n",
    "    \"\"\"\n",
    "\n",
    "    broken_trading_pairs = get_broken_pairs(\n",
    "        indicators.strategy_universe, parameters\n",
    "    )\n",
    "\n",
    "    if pair in broken_trading_pairs:\n",
    "        # Don't even bother to try trade this\n",
    "        return False\n",
    "\n",
    "    avoid_backtesting_tokens = {\n",
    "        # Trading jsut stops (though there is liq left)\n",
    "        # https://tradingstrategy.ai/trading-view/ethereum/uniswap-v3/id-usdc-fee-30\n",
    "        \"PEOPLE\",\n",
    "        \"WBTC\",\n",
    "    }\n",
    "\n",
    "    if pair.base.token_symbol in avoid_backtesting_tokens:\n",
    "        # Manually blacklisted toen for this backtest\n",
    "        return False\n",
    "\n",
    "    # Pair does not quality yet due to low liquidity\n",
    "    liquidity = indicators.get_tvl(pair=pair)\n",
    "    if liquidity is None or liquidity <= parameters.min_liquidity_threshold:\n",
    "        return False\n",
    "\n",
    "    volume = indicators.get_price(pair, column=\"volume\")\n",
    "    close_price = indicators.get_price(pair=pair)\n",
    "    if (volume is not None) and (close_price is not None):\n",
    "        volume_adjusted = abs(volume) / close_price\n",
    "        if volume_adjusted < parameters.min_volume:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "@cachetools.cached(\n",
    "    cache=cachetools.TTLCache(ttl=60 * 60 * 2, maxsize=1000),\n",
    "    key=lambda s, p: cachetools.keys.hashkey(\"get_broken_pairs\"),\n",
    ")\n",
    "def get_broken_pairs(strategy_universe: TradingStrategyUniverse, parameters) -> set:\n",
    "    # Run some extra sanity check for small cap tokens\n",
    "    broken_trading_pairs = set()\n",
    "    pairs_to_avoid = [87449]\n",
    "\n",
    "    for pair in strategy_universe.iterate_pairs():\n",
    "        reason = strategy_universe.get_trading_broken_reason(\n",
    "            pair,\n",
    "            min_candles_required=10,\n",
    "            min_price=parameters.min_price,\n",
    "            max_price=parameters.max_price,\n",
    "        )\n",
    "        if pair.internal_id in pairs_to_avoid:\n",
    "            broken_trading_pairs.add(pair)\n",
    "        if reason:\n",
    "            print(\n",
    "                f\"FAIL: {pair} with base token {pair.base.address} may be problematic: {reason}\"\n",
    "            )\n",
    "            broken_trading_pairs.add(pair)\n",
    "        else:\n",
    "            print(f\"OK: {pair} included in the backtest\")\n",
    "\n",
    "\n",
    "    return broken_trading_pairs\n",
    "\n",
    "def decide_trades(\n",
    "    input: StrategyInput,\n",
    ") -> list[TradeExecution]:\n",
    "    #\n",
    "    # Decision cycle setup.\n",
    "    # Read all variables we are going to use for the decisions.\n",
    "    #\n",
    "    parameters = input.parameters\n",
    "    position_manager = input.get_position_manager()\n",
    "    state = input.state\n",
    "    timestamp = input.timestamp\n",
    "    indicators = input.indicators\n",
    "    strategy_universe = input.strategy_universe\n",
    "\n",
    "    #Initiate logging vars\n",
    "    equity_before = state.portfolio.get_total_equity()\n",
    "    trade_decision_report = {}\n",
    "    num_signals_accepted = 0\n",
    "    num_signals_rejected = 0\n",
    "    signals_created = []\n",
    "    signals_rejected = []\n",
    "    rejected_trades_lack_of_liquidity = []\n",
    "\n",
    "    cash = position_manager.get_current_cash()\n",
    "    total_equity = state.portfolio.get_total_equity()\n",
    "    if total_equity > 10_000_000:\n",
    "        position_valuations = \"\\n\".join(\n",
    "            [\n",
    "                f\"{p} (token {p.pair.base.address}): {p.get_value()}\"\n",
    "                for p in state.portfolio.open_positions.values()\n",
    "            ]\n",
    "        )\n",
    "        raise RuntimeError(\n",
    "            f\"Portfolio total equity exceeded 1,000,000 USD. Some broken math likely happened. Total equity is {total_equity} USD.\\nOpen positions:\\n{position_valuations}\"\n",
    "        )\n",
    "\n",
    "    #\n",
    "    # Trading logic\n",
    "    #\n",
    "    # We do some extra checks here as we are trading low quality\n",
    "    # low cap tokens which often have outright malicious data for trading.\n",
    "    #\n",
    "\n",
    "    trades = []\n",
    "\n",
    "    # Enable trailing stop loss after we reach the profit taking level\n",
    "    #\n",
    "    for position in state.portfolio.open_positions.values():\n",
    "        if position.trailing_stop_loss_pct is None:\n",
    "            close_price = indicators.get_price(pair=position.pair)\n",
    "            if (\n",
    "                close_price\n",
    "                and close_price\n",
    "                >= position.get_opening_price()\n",
    "                * parameters.trailing_stop_loss_activation_level\n",
    "            ):\n",
    "                position.trailing_stop_loss_pct = parameters.trailing_stop_loss_pct\n",
    "        elif position.stop_loss is None:\n",
    "            position.stop_loss = parameters.stop_loss_pct\n",
    "\n",
    "    size_risk_model = USDTVLSizeRiskModel(\n",
    "        pricing_model=input.pricing_model,\n",
    "        per_position_cap=parameters.per_position_cap_of_pool,  # This is how much % by all pool TVL we can allocate for a position\n",
    "        missing_tvl_placeholder_usd=100_000,  # Placeholder for missing TVL data until we get the data off the chain\n",
    "    )\n",
    "\n",
    "\n",
    "    for pair in strategy_universe.iterate_pairs():\n",
    "\n",
    "        if not is_acceptable(indicators, parameters, pair):\n",
    "            # Skip this pair for the  risk management\n",
    "            continue\n",
    "\n",
    "        position_for_pair = state.portfolio.get_open_position_for_pair(pair)\n",
    "\n",
    "        # Extract Social indicators here\n",
    "        try:\n",
    "            social_mentions = indicators.get_indicator_value(\n",
    "                \"social_mentions\",\n",
    "                pair=pair,\n",
    "                index=-2,\n",
    "                data_delay_tolerance=pd.Timedelta(days=15)\n",
    "            )\n",
    "            print(f\"Social mentions for pair {pair.base.token_symbol}: {social_mentions}\")\n",
    "        except IndicatorDataNotFoundWithinDataTolerance:\n",
    "            print(\n",
    "                f\"Social mentions data not found within tolerance for pair {pair}. \"\n",
    "                \"Skipping this asset.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ema_short = indicators.get_indicator_value(\n",
    "                \"social_mentions_emas\",\n",
    "                pair=pair,\n",
    "                column=f\"social_mentions_ema_short\",\n",
    "                index=-2,\n",
    "                data_delay_tolerance=pd.Timedelta(days=15)\n",
    "            )\n",
    "        except IndicatorDataNotFoundWithinDataTolerance:\n",
    "            print(\n",
    "                f\"EMA short data not found within tolerance for pair {pair}. \"\n",
    "                \"Skipping this asset.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ema_long = indicators.get_indicator_value(\n",
    "                \"social_mentions_emas\",\n",
    "                pair=pair,\n",
    "                column=f\"social_mentions_ema_long\",\n",
    "                index=-2,\n",
    "                data_delay_tolerance=pd.Timedelta(days=15)\n",
    "            )\n",
    "        except IndicatorDataNotFoundWithinDataTolerance:\n",
    "            print(\n",
    "                f\"EMA long data not found within tolerance for pair {pair}. \"\n",
    "                \"Skipping this asset.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        ema_short_series = indicators.get_indicator_series(\n",
    "            \"social_mentions_emas\", pair=pair, column=f\"social_mentions_ema_short\"\n",
    "        )\n",
    "        ema_long_series = indicators.get_indicator_series(\n",
    "            \"social_mentions_emas\", pair=pair, column=f\"social_mentions_ema_long\"\n",
    "        )\n",
    "\n",
    "        # Volume Based Metrics\n",
    "        volume = indicators.get_price(column=\"volume\", pair=pair)\n",
    "        momentum = indicators.get_indicator_value(\"momentum\", pair=pair)\n",
    "        tvl = indicators.get_tvl(pair=pair)\n",
    "\n",
    "        crossover_occurred = False\n",
    "        try:\n",
    "            crossover, crossover_index = contains_cross_over(\n",
    "                ema_short_series,\n",
    "                ema_long_series,\n",
    "                lookback_period=parameters.cross_over_period,\n",
    "                must_return_index=True,\n",
    "            )\n",
    "            crossover_occurred = crossover and (\n",
    "                crossover_index >= -parameters.cross_over_period\n",
    "            )\n",
    "        except Exception as e:\n",
    "            crossover = None\n",
    "            crossover_occurred = False\n",
    "            print(\"Cross over did not occur due to exception: %s\", e)\n",
    "\n",
    "        #\n",
    "        # Visualisations\n",
    "        #\n",
    "\n",
    "        if input.is_visualisation_enabled():\n",
    "            visualisation = (\n",
    "                state.visualisation\n",
    "            )  \n",
    "\n",
    "            visualisation.plot_indicator(\n",
    "                timestamp,\n",
    "                f\"Social mentions {pair.base}\",\n",
    "                PlotKind.technical_indicator_detached,\n",
    "                social_mentions,\n",
    "                pair=pair,\n",
    "            )\n",
    "            visualisation.plot_indicator(\n",
    "                timestamp,\n",
    "                f\"Social mentions EMA {pair.base}\",\n",
    "                PlotKind.technical_indicator_detached,\n",
    "                ema_short,\n",
    "                pair=pair,\n",
    "            )\n",
    "            visualisation.plot_indicator(\n",
    "                timestamp,\n",
    "                f\"Social mentions Long {pair.base}\",\n",
    "                PlotKind.technical_indicator_overlay_on_detached,\n",
    "                ema_long,\n",
    "                pair=pair,\n",
    "                detached_overlay_name=f\"Social mentions EMA {pair.base}\",\n",
    "            )\n",
    "            visualisation.plot_indicator(\n",
    "                timestamp,\n",
    "                f\"Momentum {pair.base}\",\n",
    "                PlotKind.technical_indicator_detached,\n",
    "                momentum,\n",
    "                pair=pair,\n",
    "            )\n",
    "\n",
    "            trade_decision_report[pair.base.token_symbol] = {\n",
    "                \"pair\": pair.base.token_symbol,\n",
    "                \"momentum\": momentum,\n",
    "                \"social_mentions\": social_mentions,\n",
    "                \"crossover_occurred\": crossover_occurred,\n",
    "                \"momentum_above_threshold\": momentum >= parameters.minimum_mometum_threshold if momentum is not None else False,\n",
    "                \"social_mention_min_satisfied\": ema_short >= parameters.social_ma_min if ema_short is not None else False,\n",
    "                \"all_conditions_met\": (\n",
    "                    crossover_occurred \n",
    "                    and momentum is not None\n",
    "                    and momentum >= parameters.minimum_mometum_threshold\n",
    "                    and ema_short is not None \n",
    "                    and ema_short >= parameters.social_ma_min\n",
    "                )\n",
    "            }\n",
    "\n",
    "            if trade_decision_report[pair.base.token_symbol][\"all_conditions_met\"]:\n",
    "                print(f\"Accepted signal for {pair.base.token_symbol} current num_signals_accepted: {num_signals_accepted}\")\n",
    "                num_signals_accepted += 1\n",
    "                signals_created.append(trade_decision_report[pair.base.token_symbol])\n",
    "            else:\n",
    "                print(f\"Rejected signal for {pair.base.token_symbol} current num_signals_rejected: {num_signals_rejected}\")\n",
    "                num_signals_rejected += 1\n",
    "                signals_rejected.append(trade_decision_report[pair.base.token_symbol])\n",
    "\n",
    "        # Check if we are too early in the backtesting to have enough data to calculate indicators\n",
    "        # if None in (volume, bb_upper_interactions_social, bb_upper_interactions_social, bb_upper_sentiment_social, social_mentions, interactions, sentiment, sma):\n",
    "        if None in (volume, social_mentions, tvl): \n",
    "            continue\n",
    "\n",
    "        # Make sure you don't trade the same base token in current traded positions\n",
    "        open_positions = state.portfolio.open_positions.values()\n",
    "        base_token_address = pair.base.address\n",
    "        quote_token_address = pair.quote.address\n",
    "        # Check if there's already an open position with the same quote token\n",
    "        existing_position_with_quote = any(\n",
    "            pos.pair.base.address == base_token_address for pos in open_positions\n",
    "        )\n",
    "\n",
    "        # If there's already an open position with the same quote token, skip this pair\n",
    "        if existing_position_with_quote:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if (\n",
    "            len(state.portfolio.open_positions) < parameters.max_assets\n",
    "            and state.portfolio.get_open_position_for_pair(pair) is None\n",
    "        ):\n",
    "            should_open_position = False\n",
    "\n",
    "            if momentum is None:\n",
    "                if crossover_occurred and ema_short >= parameters.social_ma_min:\n",
    "                    should_open_position = True\n",
    "\n",
    "                    # if (tvl * parameters.min_liquidity_trade_threshold) <= (\n",
    "                    #     cash * parameters.allocation\n",
    "                    # ):\n",
    "                    #     buy_amount = tvl * parameters.min_liquidity_trade_threshold\n",
    "                    # else:\n",
    "                    #     buy_amount = cash * parameters.allocation\n",
    "\n",
    "                    # print(\n",
    "                    #     \"Opening position for %s with %s USDC\", pair, buy_amount\n",
    "                    # )\n",
    "\n",
    "                    # trades += position_manager.open_spot(\n",
    "                    #     pair,\n",
    "                    #     value=buy_amount,\n",
    "                    #     stop_loss_pct=parameters.stop_loss_pct,\n",
    "                    # )\n",
    "\n",
    "            elif (\n",
    "                crossover_occurred\n",
    "                and (momentum >= parameters.minimum_mometum_threshold)\n",
    "                and ema_short >= parameters.social_ma_min\n",
    "            ):\n",
    "                should_open_position = True\n",
    "\n",
    "            if should_open_position:\n",
    "                size_risk = size_risk_model.get_acceptable_size_for_position(\n",
    "                    timestamp=timestamp,\n",
    "                    pair=pair,\n",
    "                    asked_value=cash * parameters.allocation,\n",
    "                )\n",
    "                buy_amount = size_risk.accepted_size\n",
    "\n",
    "                print(\n",
    "                    \"Position size risk, pair: %s, asked: %s, accepted: %s, diagnostics: %s\",\n",
    "                    pair,\n",
    "                    size_risk.asked_size,\n",
    "                    buy_amount,\n",
    "                    size_risk.diagnostics_data,\n",
    "                )\n",
    "\n",
    "                if buy_amount >= parameters.min_trade_size_usd:\n",
    "                    print(\n",
    "                        \"Opening position for %s with %f USDC\", pair, buy_amount\n",
    "                    )\n",
    "                    trades += position_manager.open_spot(\n",
    "                        pair,\n",
    "                        value=buy_amount,\n",
    "                        stop_loss_pct=parameters.stop_loss_pct,\n",
    "                    )\n",
    "                else:\n",
    "                    rejected_trades_lack_of_liquidity.append(\n",
    "                        f\"Skipped {trade_decision_report[pair.base.token_symbol]} due to trade size {buy_amount} USD, because it is below our minimum threshold {parameters.min_trade_size_usd} USD\"\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Skipping trade size %f USD, because it is below our minimum threshold %f USD\",\n",
    "                        buy_amount,\n",
    "                        parameters.min_trade_size_usd,\n",
    "                    )\n",
    "            \n",
    "    \n",
    "    equity_after = state.portfolio.get_total_equity()\n",
    "\n",
    "    report = dedent_any(f\"\"\"\n",
    "        Trades decided: {len(trades)}\n",
    "        Pairs total: {strategy_universe.data_universe.pairs.get_count()}\n",
    "        Num Signals accepted: {num_signals_accepted}\n",
    "        Num Signals rejected: {num_signals_rejected}\n",
    "        Rejected trades due to lack of liquidity: {rejected_trades_lack_of_liquidity}\n",
    "        Total equity before: {equity_before:,.2f} USD\n",
    "        Total equity after: {equity_after:,.2f} USD\n",
    "        Cash: {position_manager.get_current_cash():,.2f} USD\n",
    "        Positions: {state.portfolio.open_positions.values()}\n",
    "        Allocated to signals: {equity_after - equity_before:,.2f} USD\n",
    "        Signals created: {signals_created}\n",
    "        Signals rejected: {signals_rejected}\n",
    "    \"\"\")\n",
    "\n",
    "    state.visualisation.add_message(timestamp, report)\n",
    "\n",
    "    return trades"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "c2j4UhgEWIKMoQTekkhLWg",
     "report_properties": {
      "rowId": "4uqLn0PZoUlkfWY1J8hhA1"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Backtest\n",
    "\n",
    "- Run the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "s6Dv4rKR62T08qF25npqoX",
     "report_properties": {
      "rowId": "IgdTIwqpWGJ1j2q4tTS7la"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from tradeexecutor.backtest.backtest_runner import run_backtest_inline\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "try:\n",
    "    result = run_backtest_inline(\n",
    "        name=parameters.id,\n",
    "        engine_version=\"0.5\",\n",
    "        decide_trades=decide_trades,\n",
    "        create_indicators=create_indicators,\n",
    "        client=client,\n",
    "        universe=strategy_universe,\n",
    "        parameters=parameters,\n",
    "        strategy_logging=False,\n",
    "        max_workers=1,\n",
    "        reserve_currency=\"usdc\", \n",
    "        start_at=parameters.backtest_start,\n",
    "        end_at=parameters.backtest_end,\n",
    "        # We need to set this really high value, because\n",
    "        # some low cap tokens may only see 1-2 trades per year\n",
    "        # and our backtesting framework aborts if it thinks\n",
    "        # there is an issue with data quality\n",
    "        data_delay_tolerance=pd.Timedelta(days=450),\n",
    "        minimum_data_lookback_range=pd.Timedelta(days=5),\n",
    "        \n",
    "        # Uncomment to enable verbose logging\n",
    "        log_level=logging.INFO,\n",
    "    )\n",
    "    state = result.state\n",
    "    trade_count = len(list(state.portfolio.get_all_trades()))\n",
    "    print(f\"Backtesting completed, backtested strategy made {trade_count} trades\")\n",
    "    print(state.portfolio.get_all_trades())\n",
    "except Exception as e:\n",
    "    print(\"error\", e)\n",
    "    print(e.__cause__)\n",
    "    raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "0D50QA9bTkOpYcI1RaIxad",
     "report_properties": {
      "rowId": "Qd7j5RFgZnz2ooXKEJwwMp"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Equity curve\n",
    "\n",
    "- Equity curve shows how your strategy accrues value over time\n",
    "- A good equity curve has a stable ascending angle\n",
    "- Benchmark against MATIC buy and hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tradeexecutor.analysis.multi_asset_benchmark import get_benchmark_data\n",
    "from tradeexecutor.visual.benchmark import visualise_equity_curve_benchmark\n",
    "\n",
    "# Pulls WMATIC/USDC as the benchmark\n",
    "benchmark_indexes = get_benchmark_data(\n",
    "    strategy_universe,\n",
    "    cumulative_with_initial_cash=state.portfolio.get_initial_cash()\n",
    ")\n",
    "\n",
    "fig = visualise_equity_curve_benchmark(\n",
    "    name=state.name,\n",
    "    portfolio_statistics=state.stats.portfolio,\n",
    "    all_cash=state.portfolio.get_initial_cash(),\n",
    "    benchmark_indexes=benchmark_indexes,\n",
    "    height=800,\n",
    "    log_y=True,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "o7h783OSkqk0GowNefAH8X",
     "report_properties": {
      "rowId": "Zx8Oqksqfex6m5V5FglUy4"
     },
     "type": "MD"
    }
   },
   "source": [
    "### Technical indicator and trade visualisation\n",
    "Draw the technical indicators we filled in in decide_trades()\n",
    "Show the made trades on the price chart for a single trading pair\n",
    "You need to zoom in to see the bollinger bands, as the default chart width is full multi-year study. However the default notebook chart mode is static images, as interactive images are a bit slow on Github Codespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tradeexecutor.visual.single_pair import visualise_single_pair\n",
    "# from tradeexecutor.visual.\n",
    "from tradingstrategy.charting.candle_chart import VolumeBarMode\n",
    "\n",
    "pairs_dict = {}\n",
    "sample_pair = None\n",
    "\n",
    "for pair in strategy_universe.iterate_pairs():\n",
    "    print(f\"Pair : {pair.internal_id} with the following description {pair.get_human_description()} and {pair.get_human_description()}\")\n",
    "    pairs_dict[pair.get_identifier] = pair\n",
    "    if pair.base.token_symbol == 'BRETT':\n",
    "        sample_pair = strategy_universe.get_trading_pair(pair.internal_id)\n",
    "        \n",
    "start_at, end_at = state.get_strategy_start_and_end()   # Limit chart to our backtesting range\n",
    "# btc_usdt = strategy_universe.get_pair_by_human_description(trading_pairs[0])\n",
    "# 68900 PEOPLE\n",
    "# 3228746 PEPE\n",
    "\n",
    "figure = visualise_single_pair(\n",
    "    state,\n",
    "    pair_id=sample_pair.internal_id,\n",
    "    execution_context=notebook_execution_context,\n",
    "    candle_universe=strategy_universe.data_universe.candles,\n",
    "    start_at=pd.Timestamp('2023-01-01'), #start_at,\n",
    "    end_at=pd.Timestamp('2024-12-01'),#end_at, #pd.Timestamp('2023-10-01'),\n",
    "    volume_bar_mode=VolumeBarMode.hidden,\n",
    "    volume_axis_name=\"Volume (USD)\",\n",
    "    height = 800,\n",
    "    title=f\"{sample_pair.base} trades\",\n",
    "    detached_indicators=True\n",
    ")\n",
    "\n",
    "# write_pickle_to_gcs(figure, \"test/eth_memecoin_fig_single_pair_latest.pkl\")\n",
    "\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "fpdKoa5tW62Rxq9cndlBVb",
     "report_properties": {
      "rowId": "xCiHQyC0KBrYNtYiODSbKy"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Performance metrics\n",
    "\n",
    "- Display portfolio performance metrics\n",
    "- Compare against buy and hold matic using the same initial capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "KPPdDbH0WNpuxE3OACrlos",
     "report_properties": {
      "rowId": "NKPRYvzxtXQd8f3VdhmkZF"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.multi_asset_benchmark import compare_strategy_backtest_to_multiple_assets\n",
    "\n",
    "compare_strategy_backtest_to_multiple_assets(\n",
    "    state,\n",
    "    strategy_universe,\n",
    "    display=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "iNC11ioKy7SPa9AvRcXElY",
     "report_properties": {
      "rowId": "Ky7zEmNSvKwi9Vd1d6KJhw"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Trading statistics\n",
    "\n",
    "- Display summare about made trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "3H0U9ryEdOlLyNCjGaD5Fj",
     "report_properties": {
      "rowId": "1vHUhqWuAfaVsrcUDa8v5l"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.trade_analyser import build_trade_analysis\n",
    "\n",
    "analysis = build_trade_analysis(state.portfolio)\n",
    "summary = analysis.calculate_summary_statistics()\n",
    "display(summary.to_dataframe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "CbrHOWavtFngrQkAXW0CRf",
     "report_properties": {
      "rowId": "R1AboN2SmAaEr3DZn6xjFQ"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Pair breakdown\n",
    "\n",
    "- Profit for each trading pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.backtest.optimiser import perform_optimisation\n",
    "from tradeexecutor.backtest.optimiser import prepare_optimiser_parameters\n",
    "from tradeexecutor.backtest.optimiser import MinTradeCountFilter\n",
    "from tradeexecutor.backtest.optimiser_functions import BalancedSharpeAndMaxDrawdownOptimisationFunction\n",
    "from tradeexecutor.backtest.optimiser_functions import optimise_profit, optimise_sharpe\n",
    "\n",
    "\n",
    "# How many Gaussian Process iterations we do\n",
    "iterations = 4\n",
    "\n",
    "# What do we optimise for\n",
    "# search_func = BalancedSharpeAndMaxDrawdownOptimisationFunction(sharpe_weight=0.7, max_drawdown_weight=0.3)\n",
    "search_func = optimise_profit\n",
    "\n",
    "optimiser_result = perform_optimisation(\n",
    "    iterations=iterations,\n",
    "    search_func=search_func,\n",
    "    decide_trades=decide_trades,\n",
    "    strategy_universe=strategy_universe,\n",
    "    parameters=prepare_optimiser_parameters(Parameters),  # Handle scikit-optimise search space\n",
    "    create_indicators=create_indicators,\n",
    "    result_filter=MinTradeCountFilter(50),\n",
    "    timeout=20*60,    \n",
    "    # Uncomment for diagnostics\n",
    "    # log_level=logging.INFO,\n",
    "    max_workers=1,\n",
    ")\n",
    "\n",
    "print(f\"Optimise completed, optimiser searched {optimiser_result.get_combination_count()} combinations, with {optimiser_result.get_cached_count()} results read directly from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.visual.grid_search import visualise_single_grid_search_result_benchmark\n",
    "\n",
    "# GridSearchResult instance that gave the best performance\n",
    "best_pick = optimiser_result.results[0].result\n",
    "\n",
    "print(f\"The best result found for {search_func} was {best_pick}\")\n",
    "\n",
    "fig = visualise_single_grid_search_result_benchmark(\n",
    "    best_pick, \n",
    "    strategy_universe, \n",
    "    initial_cash=Parameters.initial_cash,\n",
    "    log_y=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "all_positions = result.state.portfolio.get_all_positions()\n",
    "\n",
    "def calculate_profits(positions):\n",
    "    results = []\n",
    "    \n",
    "    for pos in positions:\n",
    "        buy_trade = None\n",
    "        position_id = pos.position_id\n",
    "        for trade_id, trade in pos.trades.items():\n",
    "            if trade.is_buy():\n",
    "                buy_trade = trade\n",
    "            elif trade.is_sell():\n",
    "                if buy_trade is None:\n",
    "                    continue  # Skip if we haven't found a buy trade yet\n",
    "                \n",
    "                buy_price = float(buy_trade.planned_price)\n",
    "                buy_quantity = float(buy_trade.planned_quantity)\n",
    "                sell_price = float(trade.planned_price)\n",
    "                sell_quantity = abs(float(trade.planned_quantity))  # Use abs() as sell quantity is negative\n",
    "                executed_at = trade.executed_at\n",
    "                # executed_quantity = trade.executed_quantity\n",
    "                # executed_reserve = trade.executed_reserve\n",
    "                unrealized_profit = pos.get_unrealised_and_realised_profit_percent()\n",
    "                \n",
    "                profit = (sell_price - buy_price) * sell_quantity\n",
    "                profit_percentage = (sell_price / buy_price - 1) * 100\n",
    "                \n",
    "                # Determine the type of sell\n",
    "                sell_type = \"Unknown\"\n",
    "                if trade.is_partial_take_profit:\n",
    "                    sell_type = \"Partial Take Profit\"\n",
    "                elif trade.is_stoploss():\n",
    "                    sell_type = \"Stop Loss\"\n",
    "                elif trade.planned_price:\n",
    "                    sell_type = \"Take Profit (Planned Price)\"\n",
    "\n",
    "                trade\n",
    "                \n",
    "                results.append({\n",
    "                    'Position ID': position_id,\n",
    "                    'Trade ID': trade_id,\n",
    "                    'Buy Price': buy_price,\n",
    "                    'Sell Price': sell_price,\n",
    "                    'Sell Quantity': sell_quantity,\n",
    "                    'Profit': profit,\n",
    "                    'Profit Percentage': profit_percentage,\n",
    "                    'Executed At': executed_at,\n",
    "                    # 'Executed Quantity': float(executed_quantity),\n",
    "                    # 'Executed Reserve': float(executed_reserve),\n",
    "                    'Unrealized Profit %': float(unrealized_profit),\n",
    "                    'Sell Type': sell_type,\n",
    "                    'Trade Status': trade.get_status(),\n",
    "                    'Base Token': pos.pair.base.token_symbol\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Assuming 'all_positions' is your iterable of positions\n",
    "df_profits = calculate_profits(all_positions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "KmjXoWr8Kq121sCrKPRbVv",
     "report_properties": {
      "rowId": "QD3PYhAk2wcPSeoflWDFl7"
     },
     "type": "MD"
    }
   },
   "source": [
    "- Visualise some custom indicator data, so we know it looks correct\n",
    "- We pick one of tokens in the sample data set, and pull outs it custom indicator data generated or loaded during the backtest run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "ZJD4mVb24UkpKo7jjiVE5c",
     "report_properties": {
      "rowId": "2aYpETvpre3iIwVPdgWwMN"
     },
     "type": "MD"
    }
   },
   "source": [
    "Show raw custom indicator data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "t9Dn1VNrv8ENDVZ4SeTLtR",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Example to demonstrate using a figure (This should be replaced with your actual fig object)\n",
    "# fig = go.Figure() # You would actually use your existing 'fig' from your code\n",
    "\n",
    "# Function to extract data from Plotly figure\n",
    "def extract_data_from_plotly_fig(fig):\n",
    "    data = {}\n",
    "    dates = None\n",
    "\n",
    "    for trace in fig.data:\n",
    "        if 'x' in trace and 'y' in trace:\n",
    "            if dates is None:\n",
    "                dates = trace.x  # Assuming all traces share the same 'x' dates\n",
    "            data[trace.name] = trace.y\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df.index.name = 'Date'\n",
    "    return df\n",
    "\n",
    "# Call the function with your figure\n",
    "df = extract_data_from_plotly_fig(fig)\n",
    "\n",
    "# Saving the DataFrame to CSV\n",
    "csv_path = \"gs://taraxa-research/test/extracted_trendspotting_backtest_plot_data.csv\"\n",
    "df.to_csv(csv_path)\n",
    "\n",
    "# Output to check\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "minimal",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [
    "Igzykbf6MEYFRK0y82ue40",
    "uh5o3Dn5G1utCf1pBXwLXt",
    "4GwUSsFBuYOKR3ogJzSICm",
    "I8SNSAssB6WzYun0CIP0RF",
    "mO7ovYC34GQbf5pEncLBAK",
    "leSUCsgDPZRuTbiwYxqwJo",
    "ezL0CmuhAzreuvItUBX6ib",
    "qnPd62lHQ3y3wY9nA5HHlE",
    "M34dsPMTdCIsujvuC6qlsu",
    "wK61MJFvWr3SOwjUFwbMh5",
    "zKwIFYKi3AfbRXufetZTe5",
    "LYn4zg7uquP5KqCiU61a38",
    "7qHdZrFh01nmPN6AujUGIJ",
    "qrYjxjPLKNFbrO9GBBRmyS",
    "PlFQ2UFrtKb1BRYdZIJ8SH",
    "FZSXhS5AxXpEv18zirqLFE",
    "dj5kjQzQ1hYacmyw6Mbh0K",
    "Z9YQ4OHii5m2LZuDzG4X7T",
    "swjxViRqShK0nDPy6OnSz4",
    "4uqLn0PZoUlkfWY1J8hhA1",
    "IgdTIwqpWGJ1j2q4tTS7la",
    "Sn78S9IGpKwdB2oZwZwt1M",
    "LfjkRB14OuhIb8oTy4N8uI",
    "QD3PYhAk2wcPSeoflWDFl7",
    "2aYpETvpre3iIwVPdgWwMN",
    "Zx8Oqksqfex6m5V5FglUy4",
    "SLDmFiJOJnChdr7DlmV8Ch",
    "oX72bcHrvmTjPdgf2dZFIC",
    "jPeLmxGU9WWLR9j0XucPiV",
    "Bz94JM09UFeNbhH7OfC6PL",
    "Qd7j5RFgZnz2ooXKEJwwMp",
    "JNPs9Qq7oRRETIWAY5eRrP",
    "xCiHQyC0KBrYNtYiODSbKy",
    "NKPRYvzxtXQd8f3VdhmkZF",
    "Ky7zEmNSvKwi9Vd1d6KJhw",
    "1vHUhqWuAfaVsrcUDa8v5l",
    "R1AboN2SmAaEr3DZn6xjFQ",
    "EdYfFbkiHn9hhHNyVmUdgo",
    "F3GZjzt0w7ZOgTf4eG5Vnz",
    "G6EUJkkt8kMMeG5IClBybD",
    "jsv4NscOmMfi8GjCBV2gaA",
    "GsAVRKz4cN7c34kzrqrOeG",
    "PCA6EsLlBcnclhfnHAKgV6",
    "Sfhe7ZjFfdJ9uZYJz2FeIu",
    "zrUl2nZfpOg0pFItLlBu1C",
    "3KVO1k5YBxzIppvGBKO5em",
    "DdSUDfowy9Au05iNZpBUQJ",
    "zbpxUXt1gnyKQHgBALbwov",
    "hmrHCkjFgvHyRTMvOWMBc8"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "trade-executor-gRNp9HSE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
